{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import miscnn\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "from miscnn.data_loading.interfaces.dicom_io import DICOM_interface\n",
    "from miscnn.neural_network.data_generator import DataGenerator\n",
    "from miscnn.data_loading.data_io import Data_IO \n",
    "from miscnn.utils.visualizer import normalize_volume , visualize_samples , detect_dimensionality\n",
    "from miscnn.processing.data_augmentation import Data_Augmentation\n",
    "from miscnn.processing.subfunctions import Normalization, Clipping, Resampling\n",
    "from miscnn import Preprocessor, Neural_Network\n",
    "from miscnn.neural_network.metrics import dice_soft, tversky_crossentropy,categorical_focal_loss, dice_crossentropy, asym_unified_focal_loss\n",
    "from miscnn.neural_network.architecture.unet.dense import Architecture\n",
    "\n",
    "from miscnn.evaluation.cross_validation import cross_validation, write_fold2disk, split_folds, run_fold, load_disk2fold\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, \\\n",
    "                                       EarlyStopping, CSVLogger, ModelCheckpoint\n",
    "from miscnn.evaluation import split_validation\n",
    "from IPython.display import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow and set GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a fold directory for k-fold for Cross-Validation\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "\n",
    "# Base directory for evaluation\n",
    "base_dir = \"evaluation\"\n",
    "\n",
    "# Create the base directory if it doesn't exist\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# Create subdirectories for each fold\n",
    "for fold in range(1, k_folds + 1):\n",
    "    fold_subdir = os.path.join(base_dir, \"fold_\" + str(fold))\n",
    "    if not os.path.exists(fold_subdir):\n",
    "        os.makedirs(fold_subdir)\n",
    "        print(f\"Created directory: {fold_subdir}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {fold_subdir}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DICOM interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found structures in sample : [[\"1\", 'Maxillary'], [\"2\", 'Mandible']]\n"
     ]
    }
   ],
   "source": [
    "# Create Interface\n",
    "interface = DICOM_interface(annotation_tag=\"rtstruct\")\n",
    "\n",
    "# Initialize \n",
    "data_path = \"/restricted/projectnb/ortho-ar/MinKim/data_teeth\"\n",
    "samples = interface.initialize(data_path)\n",
    "\n",
    "# Obtain ROI names\n",
    "structures = interface.get_ROI_names(samples[0])\n",
    "print('Found structures in sample : {}'.format(structures))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign Class to Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Maxillary': 1, 'Mandible': 2}\n"
     ]
    }
   ],
   "source": [
    "# Class 0 is always background class \n",
    "structure_dict = {\"Maxillary\": 1, \"Mandible\": 2}\n",
    "\n",
    "print(structure_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create New Instance of the DICOM interface with Structure Dictionary and Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = DICOM_interface(structure_dict = structure_dict, classes=3, annotation_tag=\"rtstruct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the DICOM interface to the DATA_IO class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_io = Data_IO(interface, data_path, \"/restricted/projectnb/ortho-ar/MinKim/predictions/predicted_teeth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the interface by getting a list of all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = data_io.get_indiceslist()\n",
    "sample_list.sort()\n",
    "sample_number = len(sample_list)\n",
    "print(\"All samples: \" + str(sample_list))\n",
    "print(\"Number of Samples: \" + str(sample_number))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribute Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_fold2disk(file_path= \"/restricted/projectnb/ortho-ar/MinKim/evaluation/fold_5/sample_list.json\", training = sample_list[0:247], validation = sample_list[247:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the functionality of the data pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data_io.sample_loader(samples[1])\n",
    "images, segmentations = sample.img_data, sample.seg_data\n",
    "\n",
    "print(images.shape, segmentations.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the Data Augmentation class\n",
    "data_aug = Data_Augmentation(cycles=2, scaling=True, rotations=True, elastic_deform=True, mirror=True,\n",
    "                             brightness=True, contrast=True, gamma=True, gaussian_noise=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out good voxel spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_resample = Resampling((1.8, 2, 2))\n",
    "pp_test = Preprocessor(data_io, data_aug=None, batch_size=1, subfunctions=[sf_resample], \n",
    "                       prepare_subfunctions=True, prepare_batches=False, \n",
    "                       analysis=\"fullimage\")\n",
    "\n",
    "data_gen = DataGenerator(sample_list, pp_test, training=False,\n",
    "                         shuffle=False, iterations=None)\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "for batch in data_gen:\n",
    "    x.append(batch.shape[1])\n",
    "    y.append(batch.shape[2])\n",
    "    z.append(batch.shape[3])\n",
    "\n",
    "# Clean up batch directory, afterwards\n",
    "data_io.batch_cleanup()\n",
    "\n",
    "print(np.median(x), np.median(y), np.median(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subfunctions for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pixel value normalization Subfunction through Z-Score \n",
    "sf_normalize = Normalization(mode=\"grayscale\")\n",
    "# Create a resampling Subfunction to voxel spacing 1 x 1 x 1\n",
    "sf_resample = Resampling((1, 1, 1))\n",
    "# Create a clipping Subfunction to the teeth window of CTs \n",
    "sf_clipping = Clipping(min=-300, max=3000)\n",
    "# Create a pixel value normalization subfunction for z-score scaling\n",
    "sf_zscore = Normalization(mode=\"z-score\")\n",
    "\n",
    "# Assemble Subfunction classes into a list\n",
    "# Be aware that the Subfunctions will be exectued according to the list order!\n",
    "subfunctions = [sf_clipping, sf_normalize, sf_resample, sf_zscore]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create preprocessor and Decide patchwise or full image training for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the Preprocessor class\n",
    "pp = Preprocessor(data_io, data_aug=data_aug, batch_size=3, subfunctions=subfunctions, prepare_subfunctions=True, \n",
    "                  prepare_batches=False, analysis=\"patchwise-crop\", patch_shape=(80, 160, 160))\n",
    "\n",
    "pp.patchwise_overlap = (40, 80, 80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Neural Network (DENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Architecture\n",
    "unet_standard = Architecture(depth=4, activation=\"softmax\",\n",
    "                             batch_normalization=True)\n",
    "\n",
    "# Create the Neural Network model with the legacy optimizer\n",
    "model = Neural_Network(preprocessor=pp, loss=tversky_crossentropy, metrics=[dice_soft], batch_queue_size=3, workers=3, learning_rate=0.00001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()\n",
    "model.reset_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 3D UNET Network Architecture from MIScnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miscnn.neural_network.architecture.unet.standard import Architecture\n",
    "\n",
    "# Initialize the Architecture\n",
    "unet_standard = Architecture(depth=4, activation=\"softmax\",\n",
    "                             batch_normalization=True)\n",
    "\n",
    "model = Neural_Network(preprocessor=pp, architecture=unet_standard,\n",
    "                       loss=tversky_crossentropy,\n",
    "                       metrics=[dice_soft],\n",
    "                       batch_queue_size=3, workers=3, learning_rate=0.0001)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "cb_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=20,\n",
    "                          verbose=1, mode='min', min_delta=0.0001, cooldown=1,\n",
    "                          min_lr=0.000001)\n",
    "cb_es = EarlyStopping(monitor=\"loss\", patience=50)\n",
    "cb_tb = TensorBoard(log_dir=os.path.join(fold_subdir, \"tensorboard\"),\n",
    "                    histogram_freq=0, write_graph=True, write_images=True)\n",
    "cb_cl = CSVLogger(os.path.join(fold_subdir, \"logs.csv\"), separator=',',\n",
    "                  append=True)\n",
    "cb_mc = ModelCheckpoint(os.path.join(fold_subdir, \"model.best.hdf5\"),\n",
    "                        monitor=\"loss\", verbose=1,\n",
    "                        save_best_only=True, mode=\"min\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline for CV fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------#\n",
    "#          Run Pipeline for provided CV Fold          #\n",
    "#-----------------------------------------------------#\n",
    "run_fold(fold, model, epochs=200, iterations=200, evaluation_path=\"evaluation\",\n",
    "         draw_figures=True, callbacks=[cb_lr, cb_es, cb_tb, cb_cl, cb_mc],\n",
    "         save_models=False)\n",
    "# Run pipeline for cross-validation fold\n",
    "\n",
    "# Dump latest model to disk\n",
    "model.dump(os.path.join(fold_subdir, \"model.latest.hdf5\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for provided CV fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------#\n",
    "#           Inference for provided CV Fold            #\n",
    "#-----------------------------------------------------#\n",
    "\n",
    "\n",
    "# Load best model weights during fitting\n",
    "model.load(os.path.join(fold_subdir, \"model.best.hdf5\"))\n",
    "\n",
    "# Obtain training and validation data set\n",
    "training, validation = load_disk2fold(os.path.join(fold_subdir,\n",
    "                                                   \"sample_list.json\"))\n",
    "\n",
    "# Compute predictions\n",
    "model.predict(validation, return_output=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Training/ Validation Dice Soft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Image(filename = \"evaluation/fold_5/validation.dice_soft.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Training / Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = \"evaluation/fold_5/validation.loss.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset DICOM interface with directory to data to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/restricted/projectnb/ortho-ar/MinKim/predictions/teeth_predict\"\n",
    "data_io = Data_IO(interface, data_path, \"/restricted/projectnb/ortho-ar/MinKim/predictions/predicted_teeth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = data_io.get_indiceslist()\n",
    "sample_list.sort()\n",
    "sample_number = len(sample_list)\n",
    "print(\"All samples: \" + str(sample_list))\n",
    "print(\"Number of Samples: \" + str(sample_number))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model and create prediction with 3D UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Neural Network model with the legacy optimizer\n",
    "model = Neural_Network(preprocessor=pp, loss=tversky_crossentropy, metrics=[dice_soft], batch_queue_size=3, workers=3, learning_rate=0.000001)\n",
    "\n",
    "model.load(\"/restricted/projectnb/ortho-ar/MinKim/Model/model_5.hdf5\")\n",
    "output_path = \"/restricted/projectnb/ortho-ar/MinKim/predictions/predicted_teeth\"\n",
    "model.predict(sample_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
